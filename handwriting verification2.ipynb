{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import cv2\n",
    "import glob\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras import applications\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.models import Sequential, Model, load_model\n",
    "from tensorflow.keras import applications\n",
    "from tensorflow.keras import optimizers\n",
    "from tensorflow.keras.layers import Dropout, Flatten, Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.metrics import accuracy_score\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, LearningRateScheduler, EarlyStopping, ReduceLROnPlateau, TensorBoard\n",
    "from tensorflow.keras import backend as K\n",
    "import gc\n",
    "from tensorflow.keras.models import Model\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dir=\"E:/sign_data/Dataset/train\"\n",
    "test_dir=\"E:/sign_data/Dataset/test\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "SIZE = 224\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_names = []\n",
    "test_data_names = []\n",
    "\n",
    "train_data = []\n",
    "train_labels = []\n",
    "\n",
    "for per in os.listdir('E:/sign_data/Dataset/train'):\n",
    "    for data in glob.glob('E:/sign_data/Dataset/train/'+per+'/*.*'):\n",
    "        \n",
    "        train_data_names.append(data)\n",
    "        img = cv2.imread(data)\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        img = cv2.resize(img, (SIZE,SIZE))\n",
    "        train_data.append([img])\n",
    "        if per[-1]=='g':\n",
    "            train_labels.append(np.array(1))\n",
    "        else:\n",
    "            train_labels.append(np.array(0))\n",
    "\n",
    "train_data = np.array(train_data)/255.0\n",
    "train_labels = np.array(train_labels)\n",
    "\n",
    "#Test Data\n",
    "\n",
    "test_data = []\n",
    "test_labels = []\n",
    "\n",
    "for per in os.listdir('E:/sign_data/Dataset/test'):\n",
    "    for data in glob.glob('E:/sign_data/Dataset/test/'+per+'/*.*'):\n",
    "        test_data_names.append(data)\n",
    "        img = cv2.imread(data)\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        img = cv2.resize(img, (SIZE,SIZE))\n",
    "        test_data.append([img])\n",
    "        if per[-1]=='g':\n",
    "            test_labels.append(np.array(1))\n",
    "        else:\n",
    "            test_labels.append(np.array(0))\n",
    "\n",
    "test_data = np.array(test_data)/255.0\n",
    "test_labels = np.array(test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./train2_data_names.pkl', 'wb') as fp:\n",
    "    pickle.dump(train_data_names, fp)\n",
    "\n",
    "with open('./test2_data_names.pkl', 'wb') as fp:\n",
    "    pickle.dump(test_data_names, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Categorical labels\n",
    "#print(train_labels)\n",
    "train_labels = to_categorical(train_labels)\n",
    "#print(train_data.shape)\n",
    "# Reshaping\n",
    "train_data = train_data.reshape(-1, SIZE,SIZE, 3)\n",
    "test_data = test_data.reshape(-1, SIZE,SIZE, 3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ = (224,224,3)\n",
    "EPOCHS = 40\n",
    "BS = 64\n",
    "output_ = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "vgg16_input (InputLayer)     [(None, 224, 224, 3)]     0         \n",
      "_________________________________________________________________\n",
      "vgg16 (Model)                (None, 7, 7, 512)         14714688  \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 25088)             0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 256)               6422784   \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 2)                 514       \n",
      "=================================================================\n",
      "Total params: 21,137,986\n",
      "Trainable params: 21,137,986\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 1154 samples, validate on 495 samples\n",
      "Epoch 1/40\n",
      "1154/1154 [==============================] - 1448s 1s/sample - loss: 0.7494 - accuracy: 0.5459 - val_loss: 0.7018 - val_accuracy: 0.4990\n",
      "Epoch 2/40\n",
      "1154/1154 [==============================] - 1458s 1s/sample - loss: 0.6884 - accuracy: 0.5364 - val_loss: 0.7102 - val_accuracy: 0.4990\n",
      "Epoch 3/40\n",
      "1154/1154 [==============================] - 1437s 1s/sample - loss: 0.6825 - accuracy: 0.5910 - val_loss: 0.6572 - val_accuracy: 0.6343\n",
      "Epoch 4/40\n",
      "1154/1154 [==============================] - 1456s 1s/sample - loss: 0.6415 - accuracy: 0.6629 - val_loss: 0.6631 - val_accuracy: 0.6990\n",
      "Epoch 5/40\n",
      "1154/1154 [==============================] - 1463s 1s/sample - loss: 0.6962 - accuracy: 0.5000 - val_loss: 0.6821 - val_accuracy: 0.5374\n",
      "Epoch 6/40\n",
      "1154/1154 [==============================] - 1460s 1s/sample - loss: 0.6838 - accuracy: 0.5910 - val_loss: 0.6391 - val_accuracy: 0.7515\n",
      "Epoch 7/40\n",
      "1154/1154 [==============================] - 1468s 1s/sample - loss: 0.5768 - accuracy: 0.7192 - val_loss: 0.8103 - val_accuracy: 0.6687\n",
      "Epoch 8/40\n",
      "1154/1154 [==============================] - 1510s 1s/sample - loss: 0.5742 - accuracy: 0.7634 - val_loss: 0.2525 - val_accuracy: 0.9333\n",
      "Epoch 9/40\n",
      "1154/1154 [==============================] - 1524s 1s/sample - loss: 0.2063 - accuracy: 0.9315 - val_loss: 0.1087 - val_accuracy: 0.9535\n",
      "Epoch 10/40\n",
      "1154/1154 [==============================] - 1477s 1s/sample - loss: 0.1678 - accuracy: 0.9333 - val_loss: 0.0663 - val_accuracy: 0.9758\n",
      "Epoch 11/40\n",
      "1154/1154 [==============================] - 1458s 1s/sample - loss: 0.1389 - accuracy: 0.9480 - val_loss: 0.0585 - val_accuracy: 0.9879\n",
      "Epoch 12/40\n",
      "1154/1154 [==============================] - 1453s 1s/sample - loss: 0.1121 - accuracy: 0.9515 - val_loss: 0.0717 - val_accuracy: 0.9717\n",
      "Epoch 13/40\n",
      "1154/1154 [==============================] - 1450s 1s/sample - loss: 0.0521 - accuracy: 0.9853 - val_loss: 0.0549 - val_accuracy: 0.9818\n",
      "Epoch 14/40\n",
      "1154/1154 [==============================] - 1464s 1s/sample - loss: 0.0456 - accuracy: 0.9809 - val_loss: 0.1633 - val_accuracy: 0.9333\n",
      "Epoch 15/40\n",
      "1154/1154 [==============================] - 1486s 1s/sample - loss: 0.0394 - accuracy: 0.9835 - val_loss: 0.0429 - val_accuracy: 0.9859\n",
      "Epoch 16/40\n",
      "1154/1154 [==============================] - 1517s 1s/sample - loss: 0.0275 - accuracy: 0.9896 - val_loss: 0.1194 - val_accuracy: 0.9596\n",
      "Epoch 17/40\n",
      "1154/1154 [==============================] - 1513s 1s/sample - loss: 0.0237 - accuracy: 0.9905 - val_loss: 0.1116 - val_accuracy: 0.9636\n",
      "Epoch 18/40\n",
      "1154/1154 [==============================] - 1454s 1s/sample - loss: 0.0124 - accuracy: 0.9983 - val_loss: 0.0458 - val_accuracy: 0.9899\n",
      "Epoch 19/40\n",
      "1154/1154 [==============================] - 1463s 1s/sample - loss: 0.0076 - accuracy: 0.9983 - val_loss: 0.1003 - val_accuracy: 0.9717\n",
      "Epoch 20/40\n",
      "1154/1154 [==============================] - 1437s 1s/sample - loss: 0.0047 - accuracy: 0.9983 - val_loss: 0.0856 - val_accuracy: 0.9778\n",
      "Epoch 21/40\n",
      "1154/1154 [==============================] - 1610s 1s/sample - loss: 8.8013e-04 - accuracy: 1.0000 - val_loss: 0.9878 - val_accuracy: 0.8505\n",
      "Epoch 22/40\n",
      "1154/1154 [==============================] - 1465s 1s/sample - loss: 0.3497 - accuracy: 0.8995 - val_loss: 0.0570 - val_accuracy: 0.9919\n",
      "Epoch 23/40\n",
      "1154/1154 [==============================] - 1499s 1s/sample - loss: 0.0580 - accuracy: 0.9783 - val_loss: 0.0851 - val_accuracy: 0.9697\n",
      "Epoch 24/40\n",
      "1154/1154 [==============================] - 1494s 1s/sample - loss: 0.0160 - accuracy: 0.9974 - val_loss: 0.0931 - val_accuracy: 0.9717\n",
      "Epoch 25/40\n",
      "1154/1154 [==============================] - 1453s 1s/sample - loss: 0.0054 - accuracy: 0.9991 - val_loss: 0.0659 - val_accuracy: 0.9778\n",
      "Epoch 26/40\n",
      "1154/1154 [==============================] - 1452s 1s/sample - loss: 0.0013 - accuracy: 1.0000 - val_loss: 0.0789 - val_accuracy: 0.9778\n",
      "Epoch 27/40\n",
      "1154/1154 [==============================] - 1475s 1s/sample - loss: 4.9496e-04 - accuracy: 1.0000 - val_loss: 0.1032 - val_accuracy: 0.9778\n",
      "Epoch 28/40\n",
      "1154/1154 [==============================] - 1514s 1s/sample - loss: 3.6612e-04 - accuracy: 1.0000 - val_loss: 0.1021 - val_accuracy: 0.9778\n",
      "Epoch 29/40\n",
      "1154/1154 [==============================] - 1505s 1s/sample - loss: 2.2615e-04 - accuracy: 1.0000 - val_loss: 0.1185 - val_accuracy: 0.9758\n",
      "Epoch 30/40\n",
      "1154/1154 [==============================] - 1492s 1s/sample - loss: 2.4656e-04 - accuracy: 1.0000 - val_loss: 0.1099 - val_accuracy: 0.9778\n",
      "Epoch 31/40\n",
      "1154/1154 [==============================] - 1490s 1s/sample - loss: 1.2998e-04 - accuracy: 1.0000 - val_loss: 0.1189 - val_accuracy: 0.9758\n",
      "Epoch 32/40\n",
      "1154/1154 [==============================] - 1506s 1s/sample - loss: 8.8340e-05 - accuracy: 1.0000 - val_loss: 0.1272 - val_accuracy: 0.9737\n",
      "Epoch 33/40\n",
      "1154/1154 [==============================] - 1553s 1s/sample - loss: 7.3844e-05 - accuracy: 1.0000 - val_loss: 0.1293 - val_accuracy: 0.9737\n",
      "Epoch 34/40\n",
      "1154/1154 [==============================] - 1566s 1s/sample - loss: 6.1779e-05 - accuracy: 1.0000 - val_loss: 0.1318 - val_accuracy: 0.9737\n",
      "Epoch 35/40\n",
      "1154/1154 [==============================] - 1518s 1s/sample - loss: 5.3839e-05 - accuracy: 1.0000 - val_loss: 0.1340 - val_accuracy: 0.9717\n",
      "Epoch 36/40\n",
      "1154/1154 [==============================] - 1521s 1s/sample - loss: 4.6705e-05 - accuracy: 1.0000 - val_loss: 0.1369 - val_accuracy: 0.9717\n",
      "Epoch 37/40\n",
      "1154/1154 [==============================] - 1461s 1s/sample - loss: 4.1188e-05 - accuracy: 1.0000 - val_loss: 0.1420 - val_accuracy: 0.9717\n",
      "Epoch 38/40\n",
      "1154/1154 [==============================] - 1451s 1s/sample - loss: 3.6029e-05 - accuracy: 1.0000 - val_loss: 0.1431 - val_accuracy: 0.9717\n",
      "Epoch 39/40\n",
      "1154/1154 [==============================] - 1470s 1s/sample - loss: 3.1673e-05 - accuracy: 1.0000 - val_loss: 0.1445 - val_accuracy: 0.9717\n",
      "Epoch 40/40\n",
      "1154/1154 [==============================] - 1510s 1s/sample - loss: 2.8593e-05 - accuracy: 1.0000 - val_loss: 0.1479 - val_accuracy: 0.9717\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.preprocessing.image import random_rotation\n",
    "\n",
    "base_model = applications.VGG16(weights='imagenet', include_top=False, input_shape=input_)\n",
    "\n",
    "model = Sequential()\n",
    "data_augmentation = ImageDataGenerator(rotation_range=10)\n",
    "#data_augmentation.random_transform(img_array)\n",
    "model.add(base_model)\n",
    "model.add(Flatten(input_shape=base_model.output_shape[1:]))\n",
    "model.add(Dense(256, activation='relu'))\n",
    "model.add(Dense(output_, activation='softmax'))\n",
    "\n",
    "model = Model(inputs=model.input, outputs=model.output)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=optimizers.Adam(lr=1e-4),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.summary()\n",
    "\n",
    "#earlyStopping = EarlyStopping(monitor='val_loss',\n",
    "#                             min_delta=0,\n",
    "#                             patience=3,\n",
    "#                            verbose=1)\n",
    "\n",
    "#early_stop=[earlyStopping]\n",
    "progess = model.fit(train_data,train_labels, batch_size=BS,epochs=EPOCHS, validation_split=.3)\n",
    "acc = progess.history['accuracy']\n",
    "val_acc = progess.history['val_accuracy']\n",
    "loss = progess.history['loss']\n",
    "val_loss = progess.history['val_loss']\n",
    "\n",
    "epochs = range(len(acc))\n",
    "\n",
    "plt.plot(epochs, acc, 'b', label='Training acc')\n",
    "plt.plot(epochs, val_acc, 'r', label='Validation acc')\n",
    "plt.title('Training and validation accuracy')\n",
    "plt.legend()\n",
    "plt.figure()\n",
    "plt.show()\n",
    "\n",
    "plt.plot(epochs, loss, 'b', label='Training loss')\n",
    "plt.plot(epochs, val_loss, 'r', label='Validation loss')\n",
    "plt.title('Training and validation loss')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "intermediate_layer_model = Model(inputs=model.input,\n",
    "                                 outputs=model.layers[-2].output)\n",
    "intermediate_output_train = intermediate_layer_model.predict(train_data)\n",
    "intermediate_output_test = intermediate_layer_model.predict(test_data)\n",
    "\n",
    "np.save('./train2', intermediate_output_train)\n",
    "np.save('./test2', intermediate_output_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "#model.save('forge_real_signature_model2.h5')\n",
    "\n",
    "test_output=model.predict(test_data)\n",
    "\n",
    "x=np.argmax(test_output,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "[[1.3265211e-20 1.0000000e+00]]\n"
     ]
    }
   ],
   "source": [
    "#test_output\n",
    "\n",
    "\n",
    "#pred = model.predict(test_output)\n",
    "#pred\n",
    "#img = cv2.imread('E:/sign_data/Dataset/test/049_forg/01_0114049')\n",
    "#img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "#img = cv2.resize(img, (SIZE,SIZE))\n",
    "#s=np.array(img)/255.0\n",
    "#s=s.reshape(-1, SIZE,SIZE, 3)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "# 이미지 읽기\n",
    "img = cv2.imread('E:/sign_data/Dataset/test/049_forg/01_0114049.png')\n",
    "img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "# 이미지 크기 조정\n",
    "SIZE = 224  # 예시로 224로 가정\n",
    "img = cv2.resize(img, (SIZE, SIZE))\n",
    "\n",
    "# 이미지를 모델의 입력 형태에 맞게 변환\n",
    "input_data = np.array(img) / 255.0  # 0~1 사이의 값으로 정규화\n",
    "input_data = input_data.reshape(-1, SIZE, SIZE, 3)  # 4차원 형태로 변환\n",
    "\n",
    "\n",
    "\n",
    "# 모델 예측\n",
    "prediction = model.predict(input_data)\n",
    "predicted_label = np.argmax(prediction[0])\n",
    "print(predicted_label)\n",
    "\n",
    "# 예측 수행\n",
    "pred = model.predict(input_data)\n",
    "print(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1,\n",
       "       1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1,\n",
       "       0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1,\n",
       "       1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1], dtype=int64)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "model = load_model('forge_real_signature_model2.h5')\n",
    "#test_output=model.predict(s)\n",
    "#x=np.argmax(test_output,axis=1)\n",
    "#x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved model to disk\n"
     ]
    }
   ],
   "source": [
    "# serialize model to JSON\n",
    "model_json = model.to_json()\n",
    "with open(\"model.json\", \"w\") as json_file:\n",
    "    json_file.write(model_json)\n",
    "# serialize weights to HDF5\n",
    "model.save_weights(\"model2.h5\")\n",
    "print(\"Saved model to disk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import model_from_json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model from disk\n"
     ]
    }
   ],
   "source": [
    "# load json and create model\n",
    "json_file = open('model.json', 'r')\n",
    "loaded_model_json = json_file.read()\n",
    "json_file.close()\n",
    "loaded_model = model_from_json(loaded_model_json)\n",
    "# load weights into new model\n",
    "loaded_model.load_weights(\"model2.h5\")\n",
    "print(\"Loaded model from disk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "입력 이미지는 가짜 서명입니다.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "################ 학습 모델 성능 확인#####################\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import cv2\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras import applications\n",
    "from tensorflow.keras.layers import Flatten, Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.models import Model, load_model\n",
    "\n",
    "# Load existing model\n",
    "\n",
    "SIZE = 224\n",
    "\n",
    "model = load_model('forge_real_signature_model2.h5')\n",
    "\n",
    "\n",
    "# 이미지 파일 경로\n",
    "\n",
    "image_path = 'E:/sign_data/Dataset/train/017_forg/01_0124017.png'\n",
    "\n",
    "# 이미지 로드 및 전처리\n",
    "img = cv2.imread(image_path)\n",
    "img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "img = cv2.resize(img, (SIZE, SIZE))\n",
    "img = img / 255.0\n",
    "img = np.expand_dims(img, axis=0)\n",
    "\n",
    "# 모델 예측\n",
    "prediction = model.predict(img)\n",
    "predicted_label = np.argmax(prediction[0])\n",
    "\n",
    "# 판별 결과 출력\n",
    "if predicted_label == 0:\n",
    "    print(\"입력 이미지는 진위된 서명입니다.\")\n",
    "else:\n",
    "    print(\"입력 이미지는 가짜 서명입니다.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 7 samples, validate on 3 samples\n",
      "Epoch 1/40\n",
      "7/7 [==============================] - 19s 3s/sample - loss: 26.4019 - accuracy: 0.7143 - val_loss: 27.0426 - val_accuracy: 0.0000e+00\n",
      "Epoch 2/40\n",
      "7/7 [==============================] - 15s 2s/sample - loss: 6.6976 - accuracy: 0.7143 - val_loss: 5.8960 - val_accuracy: 0.0000e+00\n",
      "Epoch 3/40\n",
      "7/7 [==============================] - 15s 2s/sample - loss: 1.2932 - accuracy: 0.7143 - val_loss: 1.6993 - val_accuracy: 0.0000e+00\n",
      "Epoch 4/40\n",
      "7/7 [==============================] - 15s 2s/sample - loss: 0.5147 - accuracy: 0.7143 - val_loss: 1.4720 - val_accuracy: 0.0000e+00\n",
      "Epoch 5/40\n",
      "7/7 [==============================] - 15s 2s/sample - loss: 0.5798 - accuracy: 0.7143 - val_loss: 1.4633 - val_accuracy: 0.0000e+00\n",
      "Epoch 6/40\n",
      "7/7 [==============================] - 15s 2s/sample - loss: 0.5883 - accuracy: 0.7143 - val_loss: 1.2595 - val_accuracy: 0.0000e+00\n",
      "Epoch 7/40\n",
      "7/7 [==============================] - 14s 2s/sample - loss: 0.5843 - accuracy: 0.7143 - val_loss: 1.1167 - val_accuracy: 0.0000e+00\n",
      "Epoch 8/40\n",
      "7/7 [==============================] - 15s 2s/sample - loss: 0.5868 - accuracy: 0.7143 - val_loss: 1.0909 - val_accuracy: 0.0000e+00\n",
      "Epoch 9/40\n",
      "7/7 [==============================] - 15s 2s/sample - loss: 0.5842 - accuracy: 0.7143 - val_loss: 1.1237 - val_accuracy: 0.0000e+00\n",
      "Epoch 10/40\n",
      "7/7 [==============================] - 15s 2s/sample - loss: 0.5761 - accuracy: 0.7143 - val_loss: 1.1946 - val_accuracy: 0.0000e+00\n",
      "Epoch 11/40\n",
      "7/7 [==============================] - 15s 2s/sample - loss: 0.5647 - accuracy: 0.7143 - val_loss: 1.2597 - val_accuracy: 0.0000e+00\n",
      "Epoch 12/40\n",
      "7/7 [==============================] - 15s 2s/sample - loss: 0.5540 - accuracy: 0.7143 - val_loss: 1.2889 - val_accuracy: 0.0000e+00\n",
      "Epoch 13/40\n",
      "7/7 [==============================] - 15s 2s/sample - loss: 0.5397 - accuracy: 0.7143 - val_loss: 1.2589 - val_accuracy: 0.0000e+00\n",
      "Epoch 14/40\n",
      "7/7 [==============================] - 15s 2s/sample - loss: 0.5213 - accuracy: 0.7143 - val_loss: 1.2109 - val_accuracy: 0.0000e+00\n",
      "Epoch 15/40\n",
      "7/7 [==============================] - 15s 2s/sample - loss: 0.5023 - accuracy: 0.7143 - val_loss: 1.2050 - val_accuracy: 0.0000e+00\n",
      "Epoch 16/40\n",
      "7/7 [==============================] - 15s 2s/sample - loss: 0.4810 - accuracy: 0.7143 - val_loss: 1.2252 - val_accuracy: 0.0000e+00\n",
      "Epoch 17/40\n",
      "7/7 [==============================] - 15s 2s/sample - loss: 0.4574 - accuracy: 0.7143 - val_loss: 1.2217 - val_accuracy: 0.0000e+00\n",
      "Epoch 18/40\n",
      "7/7 [==============================] - 15s 2s/sample - loss: 0.4315 - accuracy: 0.7143 - val_loss: 1.1710 - val_accuracy: 0.0000e+00\n",
      "Epoch 19/40\n",
      "7/7 [==============================] - 15s 2s/sample - loss: 0.4009 - accuracy: 0.7143 - val_loss: 1.0667 - val_accuracy: 0.0000e+00\n",
      "Epoch 20/40\n",
      "7/7 [==============================] - 15s 2s/sample - loss: 0.3637 - accuracy: 0.8571 - val_loss: 0.9762 - val_accuracy: 0.0000e+00\n",
      "Epoch 21/40\n",
      "7/7 [==============================] - 15s 2s/sample - loss: 0.3294 - accuracy: 0.8571 - val_loss: 0.9649 - val_accuracy: 0.0000e+00\n",
      "Epoch 22/40\n",
      "7/7 [==============================] - 15s 2s/sample - loss: 0.2860 - accuracy: 1.0000 - val_loss: 0.9426 - val_accuracy: 0.0000e+00\n",
      "Epoch 23/40\n",
      "7/7 [==============================] - 15s 2s/sample - loss: 0.2403 - accuracy: 1.0000 - val_loss: 0.8748 - val_accuracy: 0.3333\n",
      "Epoch 24/40\n",
      "7/7 [==============================] - 15s 2s/sample - loss: 0.1937 - accuracy: 1.0000 - val_loss: 0.7928 - val_accuracy: 0.3333\n",
      "Epoch 25/40\n",
      "7/7 [==============================] - 15s 2s/sample - loss: 0.1509 - accuracy: 1.0000 - val_loss: 0.7712 - val_accuracy: 0.3333\n",
      "Epoch 26/40\n",
      "7/7 [==============================] - 15s 2s/sample - loss: 0.1122 - accuracy: 1.0000 - val_loss: 0.7410 - val_accuracy: 0.3333\n",
      "Epoch 27/40\n",
      "7/7 [==============================] - 15s 2s/sample - loss: 0.0807 - accuracy: 1.0000 - val_loss: 0.6536 - val_accuracy: 0.6667\n",
      "Epoch 28/40\n",
      "7/7 [==============================] - 15s 2s/sample - loss: 0.0555 - accuracy: 1.0000 - val_loss: 0.5862 - val_accuracy: 0.6667\n",
      "Epoch 29/40\n",
      "7/7 [==============================] - 15s 2s/sample - loss: 0.0375 - accuracy: 1.0000 - val_loss: 0.5522 - val_accuracy: 0.6667\n",
      "Epoch 30/40\n",
      "7/7 [==============================] - 15s 2s/sample - loss: 0.0243 - accuracy: 1.0000 - val_loss: 0.5264 - val_accuracy: 0.6667\n",
      "Epoch 31/40\n",
      "7/7 [==============================] - 15s 2s/sample - loss: 0.0150 - accuracy: 1.0000 - val_loss: 0.5096 - val_accuracy: 0.6667\n",
      "Epoch 32/40\n",
      "7/7 [==============================] - 15s 2s/sample - loss: 0.0085 - accuracy: 1.0000 - val_loss: 0.4771 - val_accuracy: 0.6667\n",
      "Epoch 33/40\n",
      "7/7 [==============================] - 14s 2s/sample - loss: 0.0045 - accuracy: 1.0000 - val_loss: 0.4435 - val_accuracy: 0.6667\n",
      "Epoch 34/40\n",
      "7/7 [==============================] - 15s 2s/sample - loss: 0.0022 - accuracy: 1.0000 - val_loss: 0.4111 - val_accuracy: 0.6667\n",
      "Epoch 35/40\n",
      "7/7 [==============================] - 14s 2s/sample - loss: 0.0010 - accuracy: 1.0000 - val_loss: 0.3945 - val_accuracy: 0.6667\n",
      "Epoch 36/40\n",
      "7/7 [==============================] - 15s 2s/sample - loss: 4.7422e-04 - accuracy: 1.0000 - val_loss: 0.3858 - val_accuracy: 1.0000\n",
      "Epoch 37/40\n",
      "7/7 [==============================] - 15s 2s/sample - loss: 2.1809e-04 - accuracy: 1.0000 - val_loss: 0.3853 - val_accuracy: 1.0000\n",
      "Epoch 38/40\n",
      "7/7 [==============================] - 15s 2s/sample - loss: 9.6315e-05 - accuracy: 1.0000 - val_loss: 0.3948 - val_accuracy: 1.0000\n",
      "Epoch 39/40\n",
      "7/7 [==============================] - 15s 2s/sample - loss: 4.2046e-05 - accuracy: 1.0000 - val_loss: 0.4118 - val_accuracy: 1.0000\n",
      "Epoch 40/40\n",
      "7/7 [==============================] - 16s 2s/sample - loss: 1.8341e-05 - accuracy: 1.0000 - val_loss: 0.4410 - val_accuracy: 1.0000\n"
     ]
    }
   ],
   "source": [
    "############## 새로운 user의 handwriting 서명샘플에 대한 학습 모델 업데이트 ###########\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import cv2\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.layers import Flatten, Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from tensorflow.keras.models import Model, load_model\n",
    "\n",
    "# Load the model architecture from JSON file\n",
    "with open('model.json', 'r') as json_file:\n",
    "    model_json = json_file.read()\n",
    "    model = model_from_json(model_json)\n",
    "\n",
    "# Load the model weights\n",
    "model.load_weights('model2.h5')\n",
    "\n",
    "# Load and preprocess new data\n",
    "SIZE = 224\n",
    "\n",
    "new_data_names = []\n",
    "new_data = []\n",
    "new_labels = []\n",
    "\n",
    "for per in os.listdir('E:/sign_data/Dataset/train2'):\n",
    "    for data in glob.glob('E:/sign_data/Dataset/train2/'+per+'/*.*'):\n",
    "        new_data_names.append(data)\n",
    "        img = cv2.imread(data)\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        img = cv2.resize(img, (SIZE, SIZE))\n",
    "        new_data.append(img)\n",
    "        if per[-1] == 'g':\n",
    "            new_labels.append(1)\n",
    "        else:\n",
    "            new_labels.append(0)\n",
    "\n",
    "# Code for loading and preprocessing new data\n",
    "new_data = np.array(new_data) / 255.0\n",
    "new_labels = np.array(new_labels)\n",
    "\n",
    "# Convert labels to categorical\n",
    "new_labels = to_categorical(new_labels)\n",
    "\n",
    "# Reshape data\n",
    "new_data = new_data.reshape(-1, SIZE, SIZE, 3)\n",
    "\n",
    "# Specify the layers to be updated during transfer learning\n",
    "transfer_layers = 5  # Number of layers to freeze\n",
    "\n",
    "# Remove the last layer and add a new output layer\n",
    "x = model.layers[-2].output\n",
    "predictions = Dense(64, activation='relu')(x)\n",
    "predictions = Dense(2, activation='softmax')(predictions)\n",
    "transfer_model = Model(inputs=model.input, outputs=predictions)\n",
    "\n",
    "# Compile the model\n",
    "transfer_model.compile(loss='categorical_crossentropy', optimizer=Adam(lr=1e-4), metrics=['accuracy'])\n",
    "\n",
    "# Define callbacks\n",
    "\n",
    "#early_stopping = EarlyStopping(monitor='val_accuracy', patience=5, verbose=1, mode='max')\n",
    "#model_checkpoint = ModelCheckpoint('updated_model2.h5', monitor='val_accuracy', save_best_only=True, mode='max')\n",
    "\n",
    "# Train the model with new data\n",
    "EPOCHS = 40\n",
    "BS = 64\n",
    "\n",
    "progress = transfer_model.fit(new_data, new_labels, batch_size=BS, epochs=EPOCHS, validation_split=0.3)\n",
    "\n",
    "\n",
    "# Save the updated model\n",
    "transfer_model.save('updated_model2.h5')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 7 samples, validate on 3 samples\n",
      "Epoch 1/60\n",
      "7/7 [==============================] - 11s 2s/sample - loss: 2.0823 - accuracy: 0.8571 - val_loss: 1.2318e-06 - val_accuracy: 1.0000\n",
      "Epoch 2/60\n",
      "7/7 [==============================] - 9s 1s/sample - loss: 1.9962 - accuracy: 0.7143 - val_loss: 0.0059 - val_accuracy: 1.0000\n",
      "Epoch 3/60\n",
      "7/7 [==============================] - 9s 1s/sample - loss: 0.2480 - accuracy: 0.8571 - val_loss: 0.1215 - val_accuracy: 1.0000\n",
      "Epoch 4/60\n",
      "7/7 [==============================] - 9s 1s/sample - loss: 0.0460 - accuracy: 1.0000 - val_loss: 0.5287 - val_accuracy: 0.6667\n",
      "Epoch 5/60\n",
      "7/7 [==============================] - 9s 1s/sample - loss: 0.1211 - accuracy: 1.0000 - val_loss: 0.7869 - val_accuracy: 0.6667\n",
      "Epoch 6/60\n",
      "7/7 [==============================] - 9s 1s/sample - loss: 0.1388 - accuracy: 1.0000 - val_loss: 0.7760 - val_accuracy: 0.6667\n",
      "Epoch 7/60\n",
      "7/7 [==============================] - 9s 1s/sample - loss: 0.0848 - accuracy: 1.0000 - val_loss: 0.6664 - val_accuracy: 0.6667\n",
      "Epoch 8/60\n",
      "7/7 [==============================] - 9s 1s/sample - loss: 0.0384 - accuracy: 1.0000 - val_loss: 0.5362 - val_accuracy: 0.6667\n",
      "Epoch 9/60\n",
      "7/7 [==============================] - 9s 1s/sample - loss: 0.0182 - accuracy: 1.0000 - val_loss: 0.3863 - val_accuracy: 0.6667\n",
      "Epoch 10/60\n",
      "7/7 [==============================] - 9s 1s/sample - loss: 0.0089 - accuracy: 1.0000 - val_loss: 0.2497 - val_accuracy: 0.6667\n",
      "Epoch 11/60\n",
      "7/7 [==============================] - 9s 1s/sample - loss: 0.0039 - accuracy: 1.0000 - val_loss: 0.1566 - val_accuracy: 1.0000\n",
      "Epoch 12/60\n",
      "7/7 [==============================] - 9s 1s/sample - loss: 0.0015 - accuracy: 1.0000 - val_loss: 0.0937 - val_accuracy: 1.0000\n",
      "Epoch 13/60\n",
      "7/7 [==============================] - 10s 1s/sample - loss: 6.6467e-04 - accuracy: 1.0000 - val_loss: 0.0569 - val_accuracy: 1.0000\n",
      "Epoch 14/60\n",
      "7/7 [==============================] - 9s 1s/sample - loss: 3.2174e-04 - accuracy: 1.0000 - val_loss: 0.0393 - val_accuracy: 1.0000\n",
      "Epoch 15/60\n",
      "7/7 [==============================] - 9s 1s/sample - loss: 1.6192e-04 - accuracy: 1.0000 - val_loss: 0.0293 - val_accuracy: 1.0000\n",
      "Epoch 16/60\n",
      "7/7 [==============================] - 9s 1s/sample - loss: 8.4205e-05 - accuracy: 1.0000 - val_loss: 0.0223 - val_accuracy: 1.0000\n",
      "Epoch 17/60\n",
      "7/7 [==============================] - 9s 1s/sample - loss: 4.6212e-05 - accuracy: 1.0000 - val_loss: 0.0173 - val_accuracy: 1.0000\n",
      "Epoch 18/60\n",
      "7/7 [==============================] - 9s 1s/sample - loss: 2.6496e-05 - accuracy: 1.0000 - val_loss: 0.0133 - val_accuracy: 1.0000\n",
      "Epoch 19/60\n",
      "7/7 [==============================] - 10s 1s/sample - loss: 1.6075e-05 - accuracy: 1.0000 - val_loss: 0.0104 - val_accuracy: 1.0000\n",
      "Epoch 20/60\n",
      "7/7 [==============================] - 9s 1s/sample - loss: 1.0575e-05 - accuracy: 1.0000 - val_loss: 0.0081 - val_accuracy: 1.0000\n",
      "Epoch 21/60\n",
      "7/7 [==============================] - 9s 1s/sample - loss: 7.1524e-06 - accuracy: 1.0000 - val_loss: 0.0063 - val_accuracy: 1.0000\n",
      "Epoch 22/60\n",
      "7/7 [==============================] - 9s 1s/sample - loss: 4.9556e-06 - accuracy: 1.0000 - val_loss: 0.0050 - val_accuracy: 1.0000\n",
      "Epoch 23/60\n",
      "7/7 [==============================] - 9s 1s/sample - loss: 3.4911e-06 - accuracy: 1.0000 - val_loss: 0.0039 - val_accuracy: 1.0000\n",
      "Epoch 24/60\n",
      "7/7 [==============================] - 9s 1s/sample - loss: 2.5034e-06 - accuracy: 1.0000 - val_loss: 0.0031 - val_accuracy: 1.0000\n",
      "Epoch 25/60\n",
      "7/7 [==============================] - 9s 1s/sample - loss: 1.8392e-06 - accuracy: 1.0000 - val_loss: 0.0025 - val_accuracy: 1.0000\n",
      "Epoch 26/60\n",
      "7/7 [==============================] - 13s 2s/sample - loss: 1.3794e-06 - accuracy: 1.0000 - val_loss: 0.0021 - val_accuracy: 1.0000\n",
      "Epoch 27/60\n",
      "7/7 [==============================] - 16s 2s/sample - loss: 1.0388e-06 - accuracy: 1.0000 - val_loss: 0.0018 - val_accuracy: 1.0000\n",
      "Epoch 28/60\n",
      "7/7 [==============================] - 16s 2s/sample - loss: 8.1743e-07 - accuracy: 1.0000 - val_loss: 0.0015 - val_accuracy: 1.0000\n",
      "Epoch 29/60\n",
      "7/7 [==============================] - 15s 2s/sample - loss: 6.4713e-07 - accuracy: 1.0000 - val_loss: 0.0013 - val_accuracy: 1.0000\n",
      "Epoch 30/60\n",
      "7/7 [==============================] - 16s 2s/sample - loss: 5.1090e-07 - accuracy: 1.0000 - val_loss: 0.0011 - val_accuracy: 1.0000\n",
      "Epoch 31/60\n",
      "7/7 [==============================] - 15s 2s/sample - loss: 4.2575e-07 - accuracy: 1.0000 - val_loss: 9.7895e-04 - val_accuracy: 1.0000\n",
      "Epoch 32/60\n",
      "7/7 [==============================] - 16s 2s/sample - loss: 3.4060e-07 - accuracy: 1.0000 - val_loss: 8.7188e-04 - val_accuracy: 1.0000\n",
      "Epoch 33/60\n",
      "7/7 [==============================] - 16s 2s/sample - loss: 2.8951e-07 - accuracy: 1.0000 - val_loss: 7.8297e-04 - val_accuracy: 1.0000\n",
      "Epoch 34/60\n",
      "7/7 [==============================] - 15s 2s/sample - loss: 2.5545e-07 - accuracy: 1.0000 - val_loss: 7.0799e-04 - val_accuracy: 1.0000\n",
      "Epoch 35/60\n",
      "7/7 [==============================] - 14s 2s/sample - loss: 2.2139e-07 - accuracy: 1.0000 - val_loss: 6.4466e-04 - val_accuracy: 1.0000\n",
      "Epoch 36/60\n",
      "7/7 [==============================] - 14s 2s/sample - loss: 1.8733e-07 - accuracy: 1.0000 - val_loss: 5.8778e-04 - val_accuracy: 1.0000\n",
      "Epoch 37/60\n",
      "7/7 [==============================] - 14s 2s/sample - loss: 1.7030e-07 - accuracy: 1.0000 - val_loss: 5.3903e-04 - val_accuracy: 1.0000\n",
      "Epoch 38/60\n",
      "7/7 [==============================] - 14s 2s/sample - loss: 1.5327e-07 - accuracy: 1.0000 - val_loss: 4.9820e-04 - val_accuracy: 1.0000\n",
      "Epoch 39/60\n",
      "7/7 [==============================] - 12s 2s/sample - loss: 1.3624e-07 - accuracy: 1.0000 - val_loss: 4.6436e-04 - val_accuracy: 1.0000\n",
      "Epoch 40/60\n",
      "7/7 [==============================] - 10s 1s/sample - loss: 1.1921e-07 - accuracy: 1.0000 - val_loss: 4.3495e-04 - val_accuracy: 1.0000\n",
      "Epoch 41/60\n",
      "7/7 [==============================] - 9s 1s/sample - loss: 1.0218e-07 - accuracy: 1.0000 - val_loss: 4.0927e-04 - val_accuracy: 1.0000\n",
      "Epoch 42/60\n",
      "7/7 [==============================] - 8s 1s/sample - loss: 1.0218e-07 - accuracy: 1.0000 - val_loss: 3.8677e-04 - val_accuracy: 1.0000\n",
      "Epoch 43/60\n",
      "7/7 [==============================] - 8s 1s/sample - loss: 1.0218e-07 - accuracy: 1.0000 - val_loss: 3.6684e-04 - val_accuracy: 1.0000\n",
      "Epoch 44/60\n",
      "7/7 [==============================] - 8s 1s/sample - loss: 8.5149e-08 - accuracy: 1.0000 - val_loss: 3.4934e-04 - val_accuracy: 1.0000\n",
      "Epoch 45/60\n",
      "7/7 [==============================] - 8s 1s/sample - loss: 8.5149e-08 - accuracy: 1.0000 - val_loss: 3.3390e-04 - val_accuracy: 1.0000\n",
      "Epoch 46/60\n",
      "7/7 [==============================] - 8s 1s/sample - loss: 8.5149e-08 - accuracy: 1.0000 - val_loss: 3.2016e-04 - val_accuracy: 1.0000\n",
      "Epoch 47/60\n",
      "7/7 [==============================] - 8s 1s/sample - loss: 6.8120e-08 - accuracy: 1.0000 - val_loss: 3.0786e-04 - val_accuracy: 1.0000\n",
      "Epoch 48/60\n",
      "7/7 [==============================] - 8s 1s/sample - loss: 6.8120e-08 - accuracy: 1.0000 - val_loss: 2.9690e-04 - val_accuracy: 1.0000\n",
      "Epoch 49/60\n",
      "7/7 [==============================] - 8s 1s/sample - loss: 6.8120e-08 - accuracy: 1.0000 - val_loss: 2.8701e-04 - val_accuracy: 1.0000\n",
      "Epoch 50/60\n",
      "7/7 [==============================] - 8s 1s/sample - loss: 6.8120e-08 - accuracy: 1.0000 - val_loss: 2.7812e-04 - val_accuracy: 1.0000\n",
      "Epoch 51/60\n",
      "7/7 [==============================] - 8s 1s/sample - loss: 6.8120e-08 - accuracy: 1.0000 - val_loss: 2.7002e-04 - val_accuracy: 1.0000\n",
      "Epoch 52/60\n",
      "7/7 [==============================] - 8s 1s/sample - loss: 6.8120e-08 - accuracy: 1.0000 - val_loss: 2.6271e-04 - val_accuracy: 1.0000\n",
      "Epoch 53/60\n",
      "7/7 [==============================] - 8s 1s/sample - loss: 5.1090e-08 - accuracy: 1.0000 - val_loss: 2.5608e-04 - val_accuracy: 1.0000\n",
      "Epoch 54/60\n",
      "7/7 [==============================] - 8s 1s/sample - loss: 5.1090e-08 - accuracy: 1.0000 - val_loss: 2.5005e-04 - val_accuracy: 1.0000\n",
      "Epoch 55/60\n",
      "7/7 [==============================] - 8s 1s/sample - loss: 5.1090e-08 - accuracy: 1.0000 - val_loss: 2.4453e-04 - val_accuracy: 1.0000\n",
      "Epoch 56/60\n",
      "7/7 [==============================] - 8s 1s/sample - loss: 5.1090e-08 - accuracy: 1.0000 - val_loss: 2.3953e-04 - val_accuracy: 1.0000\n",
      "Epoch 57/60\n",
      "7/7 [==============================] - 8s 1s/sample - loss: 5.1090e-08 - accuracy: 1.0000 - val_loss: 2.3480e-04 - val_accuracy: 1.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 58/60\n",
      "7/7 [==============================] - 8s 1s/sample - loss: 5.1090e-08 - accuracy: 1.0000 - val_loss: 2.3055e-04 - val_accuracy: 1.0000\n",
      "Epoch 59/60\n",
      "7/7 [==============================] - 8s 1s/sample - loss: 5.1090e-08 - accuracy: 1.0000 - val_loss: 2.2662e-04 - val_accuracy: 1.0000\n",
      "Epoch 60/60\n",
      "7/7 [==============================] - 9s 1s/sample - loss: 5.1090e-08 - accuracy: 1.0000 - val_loss: 2.2293e-04 - val_accuracy: 1.0000\n"
     ]
    }
   ],
   "source": [
    "############## 새로운 user의 handwriting 서명샘플에 대한 학습 모델 업데이트 ###########\n",
    "\n",
    "\n",
    "############## final_test\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import cv2\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.layers import Flatten, Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from tensorflow.keras.models import Model, load_model\n",
    "\n",
    "model = load_model('forge_real_signature_model2.h5')\n",
    "# Load and preprocess new data\n",
    "SIZE = 224\n",
    "\n",
    "new_data_names = []\n",
    "new_data = []\n",
    "new_labels = []\n",
    "\n",
    "for per in os.listdir('E:/sign_data/Dataset/train2'):\n",
    "    for data in glob.glob('E:/sign_data/Dataset/train2/'+per+'/*.*'):\n",
    "        new_data_names.append(data)\n",
    "        img = cv2.imread(data)\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        img = cv2.resize(img, (SIZE, SIZE))\n",
    "        new_data.append(img)\n",
    "        if per[-1] == 'g':\n",
    "            new_labels.append(1)\n",
    "        else:\n",
    "            new_labels.append(0)\n",
    "\n",
    "# Code for loading and preprocessing new data\n",
    "new_data = np.array(new_data) / 255.0\n",
    "new_labels = np.array(new_labels)\n",
    "\n",
    "# Convert labels to categorical\n",
    "new_labels = to_categorical(new_labels)\n",
    "\n",
    "# Reshape data\n",
    "new_data = new_data.reshape(-1, SIZE, SIZE, 3)\n",
    "\n",
    "# Specify the layers to be updated during transfer learning\n",
    "transfer_layers = 5  # Number of layers to freeze\n",
    "\n",
    "# Remove the last layer and add a new output layer\n",
    "x = model.layers[-2].output\n",
    "predictions = Dense(64, activation='relu')(x)\n",
    "predictions = Dense(2, activation='softmax')(predictions)\n",
    "transfer_model = Model(inputs=model.input, outputs=predictions)\n",
    "\n",
    "# Compile the model\n",
    "transfer_model.compile(loss='categorical_crossentropy', optimizer=Adam(lr=1e-4), metrics=['accuracy'])\n",
    "\n",
    "# Define callbacks\n",
    "\n",
    "#early_stopping = EarlyStopping(monitor='val_accuracy', patience=5, verbose=1, mode='max')\n",
    "#model_checkpoint = ModelCheckpoint('updated_model2.h5', monitor='val_accuracy', save_best_only=True, mode='max')\n",
    "\n",
    "# Train the model with new data\n",
    "EPOCHS = 60\n",
    "BS = 64\n",
    "\n",
    "progress = transfer_model.fit(new_data, new_labels, batch_size=BS, epochs=EPOCHS, validation_split=0.3)\n",
    "\n",
    "\n",
    "# Save the updated model\n",
    "transfer_model.save('updated_model4.h5')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data: E:/sign_data/Dataset/train2/001\\KakaoTalk_20230603_202541998.jpg, Authenticity: Real\n",
      "Data: E:/sign_data/Dataset/train2/001\\KakaoTalk_20230603_202541998_01.jpg, Authenticity: Real\n",
      "Data: E:/sign_data/Dataset/train2/001\\KakaoTalk_20230603_202541998_02.jpg, Authenticity: Real\n",
      "Data: E:/sign_data/Dataset/train2/001\\KakaoTalk_20230603_202541998_03.jpg, Authenticity: Real\n",
      "Data: E:/sign_data/Dataset/train2/001\\KakaoTalk_20230603_202541998_04.jpg, Authenticity: Real\n",
      "Data: E:/sign_data/Dataset/train2/001_forg\\10.jpg, Authenticity: Forgery\n",
      "Data: E:/sign_data/Dataset/train2/001_forg\\6.jpg, Authenticity: Forgery\n",
      "Data: E:/sign_data/Dataset/train2/001_forg\\7.jpg, Authenticity: Forgery\n",
      "Data: E:/sign_data/Dataset/train2/001_forg\\8.jpg, Authenticity: Forgery\n",
      "Data: E:/sign_data/Dataset/train2/001_forg\\9.jpg, Authenticity: Forgery\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "###################### 추가 데이터 결과 확인 ######################\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import cv2\n",
    "from tensorflow.keras.models import Model, load_model\n",
    "\n",
    "# Load existing model\n",
    "model = load_model('updated_model4.h5')\n",
    "\n",
    "# Load and preprocess new data\n",
    "SIZE = 224\n",
    "\n",
    "new_data_names = []\n",
    "new_data = []\n",
    "\n",
    "for per in os.listdir('E:/sign_data/Dataset/train2'):\n",
    "    for data in glob.glob('E:/sign_data/Dataset/train2/'+per+'/*.*'):\n",
    "        new_data_names.append(data)\n",
    "        img = cv2.imread(data)\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        img = cv2.resize(img, (SIZE, SIZE))\n",
    "        new_data.append(img)\n",
    "\n",
    "# Code for loading and preprocessing new data\n",
    "new_data = np.array(new_data) / 255.0\n",
    "\n",
    "# Reshape data\n",
    "new_data = new_data.reshape(-1, SIZE, SIZE, 3)\n",
    "\n",
    "# Predict the authenticity of the new data\n",
    "predictions = model.predict(new_data)\n",
    "authenticity = np.argmax(predictions, axis=1)\n",
    "\n",
    "# Display the results\n",
    "for i, data_name in enumerate(new_data_names):\n",
    "    print(f\"Data: {data_name}, Authenticity: {'Real' if authenticity[i] == 0 else 'Forgery'}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Signature authenticity:  ('Forgery', 0.0008924822395783849)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "################## new input verification #################\n",
    "\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "# Load the updated model\n",
    "model = load_model('updated_model2.h5')\n",
    "\n",
    "# Load and preprocess the signature image\n",
    "SIZE = 224\n",
    "\n",
    "# Function to preprocess the image\n",
    "def preprocess_image(image_path):\n",
    "    img = cv2.imread(image_path)\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    img = cv2.resize(img, (SIZE, SIZE))\n",
    "    img = np.array(img) / 255.0\n",
    "    img = img.reshape(-1, SIZE, SIZE, 3)\n",
    "    return img\n",
    "\n",
    "# Function to predict the authenticity of a signature\n",
    "def predict_signature(image_path):\n",
    "    preprocessed_image = preprocess_image(image_path)\n",
    "    predictions = model.predict(preprocessed_image)\n",
    "    authenticity = \"Real\" if predictions[0][0] > predictions[0][1] and predictions[0][0] > 0.85  else \"Forgery\"\n",
    "    confidence = predictions[0][0] * 100\n",
    "    #print(predictions[0][0])\n",
    "    #print(predictions[0][1])\n",
    "    \n",
    "    #print(predictions)\n",
    "    return authenticity,confidence\n",
    "\n",
    "# Path to the signature image\n",
    "\n",
    "#signature_image_path = 'E:/sign_data/Dataset/train1/001/5.png'\n",
    "\n",
    "signature_image_path = 'E:/sign_data/Dataset/train2/forg_test1.jpg'\n",
    "\n",
    "#signature_image_path = 'E:/sign_data/Dataset/train2/forg_test_1.jpg'\n",
    "\n",
    "#signature_image_path = 'E:/sign_data/Dataset/KakaoTalk_20230528_151816148.png'\n",
    "\n",
    "#signature_image_path = 'E:/sign_data/KakaoTalk_20230528_152937618.jpg'\n",
    "\n",
    "# Predict the authenticity of the signature\n",
    "authenticity = predict_signature(signature_image_path)\n",
    "\n",
    "\n",
    "\n",
    "# Print the result\n",
    "print(\"Signature authenticity: \", authenticity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "####### 모델 경량화 ###############\n",
    "\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "# Load the model\n",
    "model = load_model('updated_model2.h5')\n",
    "\n",
    "# Save model architecture as JSON\n",
    "model_json = model.to_json()\n",
    "with open(\"updated_model2.json\", \"w\") as json_file:\n",
    "    json_file.write(model_json)\n",
    "\n",
    "# Save model weights as HDF5\n",
    "model.save_weights(\"updated_model2_weights.h5\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model converted to TensorFlow Lite format and saved as updated_model2.tflite\n"
     ]
    }
   ],
   "source": [
    "######## tfLite model #######################\n",
    "\n",
    "import tensorflow as tf\n",
    "import pickle\n",
    "\n",
    "# Load the updated model\n",
    "model = tf.keras.models.load_model('updated_model2.h5')\n",
    "\n",
    "# Convert the model to TensorFlow Lite format\n",
    "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
    "tflite_model = converter.convert()\n",
    "\n",
    "# Save the TensorFlow Lite model\n",
    "with open('updated_model2.tflite', 'wb') as f:\n",
    "    f.write(tflite_model)\n",
    "\n",
    "# Save the label mapping\n",
    "label_mapping = {\n",
    "    0: 'Real',\n",
    "    1: 'Forgery'\n",
    "}\n",
    "\n",
    "with open('label_mapping.pkl', 'wb') as f:\n",
    "    pickle.dump(label_mapping, f)\n",
    "\n",
    "print(\"Model converted to TensorFlow Lite format and saved as updated_model2.tflite\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pip in c:\\users\\82102\\anaconda3\\lib\\site-packages (23.0.1)\n",
      "Collecting pip\n",
      "  Downloading pip-23.1.2-py3-none-any.whl (2.1 MB)\n",
      "     ---------------------------------------- 2.1/2.1 MB 1.7 MB/s eta 0:00:00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: To modify pip, please run the following command:\n",
      "c:\\users\\82102\\anaconda3\\python.exe -m pip install --upgrade pip\n",
      "\n",
      "[notice] A new release of pip is available: 23.0.1 -> 23.1.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade pip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting jax\n",
      "  Using cached jax-0.3.25.tar.gz (1.1 MB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'error'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  error: subprocess-exited-with-error\n",
      "  \n",
      "  python setup.py egg_info did not run successfully.\n",
      "  exit code: 1\n",
      "  \n",
      "  [6 lines of output]\n",
      "  Traceback (most recent call last):\n",
      "    File \"<string>\", line 36, in <module>\n",
      "    File \"<pip-setuptools-caller>\", line 34, in <module>\n",
      "    File \"C:\\Users\\82102\\AppData\\Local\\Temp\\pip-install-idash5tb\\jax_c10acd824a894c71bda70602b18997f7\\setup.py\", line 38, in <module>\n",
      "      _long_description = f.read()\n",
      "  UnicodeDecodeError: 'cp949' codec can't decode byte 0xe2 in position 1304: illegal multibyte sequence\n",
      "  [end of output]\n",
      "  \n",
      "  note: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "error: metadata-generation-failed\n",
      "\n",
      "Encountered error while generating package metadata.\n",
      "\n",
      "See above for output.\n",
      "\n",
      "note: This is an issue with the package mentioned above, not pip.\n",
      "hint: See above for details.\n",
      "\n",
      "[notice] A new release of pip is available: 23.0.1 -> 23.1.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install --pre -U jax jaxlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflowjs'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-49-4a580e2aeb1a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m###################  tfjs model ##########################\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mtensorflowjs\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mtfjs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;31m# Load the TensorFlow model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'tensorflowjs'"
     ]
    }
   ],
   "source": [
    "###################  tfjs model ##########################\n",
    "\n",
    "import tensorflowjs as tfjs\n",
    "\n",
    "# Load the TensorFlow model\n",
    "model = tf.keras.models.load_model('updated_model2.h5')\n",
    "\n",
    "# Convert the model to TensorFlow.js format\n",
    "tfjs.converters.save_keras_model(model, 'updated_model2.tfjs')\n",
    "\n",
    "print(\"Model converted to TensorFlow.js format and saved as your_tfjs_model\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 7 samples, validate on 3 samples\n",
      "Epoch 1/30\n",
      "7/7 [==============================] - 22s 3s/sample - loss: 20.6905 - accuracy: 0.2857 - val_loss: 4.1091 - val_accuracy: 0.6667\n",
      "Epoch 2/30\n",
      "7/7 [==============================] - 17s 2s/sample - loss: 3.1842 - accuracy: 0.7143 - val_loss: 3.8134 - val_accuracy: 0.6667\n",
      "Epoch 3/30\n",
      "7/7 [==============================] - 17s 2s/sample - loss: 2.1775 - accuracy: 0.7143 - val_loss: 2.5786 - val_accuracy: 0.3333\n",
      "Epoch 4/30\n",
      "7/7 [==============================] - 16s 2s/sample - loss: 0.8244 - accuracy: 0.7143 - val_loss: 1.7166 - val_accuracy: 0.3333\n",
      "Epoch 5/30\n",
      "7/7 [==============================] - 15s 2s/sample - loss: 0.2641 - accuracy: 0.8571 - val_loss: 1.2063 - val_accuracy: 0.0000e+00\n",
      "Epoch 6/30\n",
      "7/7 [==============================] - 15s 2s/sample - loss: 0.5110 - accuracy: 0.7143 - val_loss: 1.9268 - val_accuracy: 0.0000e+00\n",
      "Epoch 7/30\n",
      "7/7 [==============================] - 15s 2s/sample - loss: 0.5417 - accuracy: 0.7143 - val_loss: 2.2717 - val_accuracy: 0.0000e+00\n",
      "Epoch 8/30\n",
      "7/7 [==============================] - 14s 2s/sample - loss: 0.5202 - accuracy: 0.7143 - val_loss: 2.2317 - val_accuracy: 0.0000e+00\n",
      "Epoch 9/30\n",
      "7/7 [==============================] - 14s 2s/sample - loss: 0.5128 - accuracy: 0.7143 - val_loss: 1.9563 - val_accuracy: 0.0000e+00\n",
      "Epoch 10/30\n",
      "7/7 [==============================] - 15s 2s/sample - loss: 0.4789 - accuracy: 0.7143 - val_loss: 1.6356 - val_accuracy: 0.0000e+00\n",
      "Epoch 11/30\n",
      "7/7 [==============================] - 15s 2s/sample - loss: 0.4489 - accuracy: 0.7143 - val_loss: 1.3983 - val_accuracy: 0.0000e+00\n",
      "Epoch 12/30\n",
      "7/7 [==============================] - 15s 2s/sample - loss: 0.4339 - accuracy: 0.8571 - val_loss: 1.2948 - val_accuracy: 0.0000e+00\n",
      "Epoch 13/30\n",
      "7/7 [==============================] - 15s 2s/sample - loss: 0.4142 - accuracy: 1.0000 - val_loss: 1.3379 - val_accuracy: 0.0000e+00\n",
      "Epoch 14/30\n",
      "7/7 [==============================] - 15s 2s/sample - loss: 0.3732 - accuracy: 1.0000 - val_loss: 1.4904 - val_accuracy: 0.0000e+00\n",
      "Epoch 15/30\n",
      "7/7 [==============================] - 15s 2s/sample - loss: 0.3232 - accuracy: 1.0000 - val_loss: 1.6870 - val_accuracy: 0.0000e+00\n",
      "Epoch 16/30\n",
      "7/7 [==============================] - 14s 2s/sample - loss: 0.2782 - accuracy: 1.0000 - val_loss: 1.9023 - val_accuracy: 0.0000e+00\n",
      "Epoch 17/30\n",
      "7/7 [==============================] - 15s 2s/sample - loss: 0.2467 - accuracy: 0.8571 - val_loss: 2.1057 - val_accuracy: 0.0000e+00\n",
      "Epoch 18/30\n",
      "7/7 [==============================] - 15s 2s/sample - loss: 0.2197 - accuracy: 0.8571 - val_loss: 2.2480 - val_accuracy: 0.0000e+00\n",
      "Epoch 19/30\n",
      "7/7 [==============================] - 15s 2s/sample - loss: 0.1910 - accuracy: 0.8571 - val_loss: 2.3311 - val_accuracy: 0.0000e+00\n",
      "Epoch 20/30\n",
      "7/7 [==============================] - 15s 2s/sample - loss: 0.1596 - accuracy: 1.0000 - val_loss: 2.3553 - val_accuracy: 0.0000e+00\n",
      "Epoch 21/30\n",
      "7/7 [==============================] - 15s 2s/sample - loss: 0.1264 - accuracy: 1.0000 - val_loss: 2.3330 - val_accuracy: 0.0000e+00\n",
      "Epoch 22/30\n",
      "7/7 [==============================] - 15s 2s/sample - loss: 0.0975 - accuracy: 1.0000 - val_loss: 2.3031 - val_accuracy: 0.0000e+00\n",
      "Epoch 23/30\n",
      "7/7 [==============================] - 15s 2s/sample - loss: 0.0749 - accuracy: 1.0000 - val_loss: 2.2843 - val_accuracy: 0.0000e+00\n",
      "Epoch 24/30\n",
      "7/7 [==============================] - 15s 2s/sample - loss: 0.0567 - accuracy: 1.0000 - val_loss: 2.3057 - val_accuracy: 0.0000e+00\n",
      "Epoch 25/30\n",
      "7/7 [==============================] - 15s 2s/sample - loss: 0.0418 - accuracy: 1.0000 - val_loss: 2.3618 - val_accuracy: 0.0000e+00\n",
      "Epoch 26/30\n",
      "7/7 [==============================] - 15s 2s/sample - loss: 0.0307 - accuracy: 1.0000 - val_loss: 2.4426 - val_accuracy: 0.0000e+00\n",
      "Epoch 27/30\n",
      "7/7 [==============================] - 15s 2s/sample - loss: 0.0224 - accuracy: 1.0000 - val_loss: 2.5520 - val_accuracy: 0.0000e+00\n",
      "Epoch 28/30\n",
      "7/7 [==============================] - 15s 2s/sample - loss: 0.0161 - accuracy: 1.0000 - val_loss: 2.6947 - val_accuracy: 0.0000e+00\n",
      "Epoch 29/30\n",
      "7/7 [==============================] - 15s 2s/sample - loss: 0.0114 - accuracy: 1.0000 - val_loss: 2.8611 - val_accuracy: 0.0000e+00\n",
      "Epoch 30/30\n",
      "7/7 [==============================] - 15s 2s/sample - loss: 0.0079 - accuracy: 1.0000 - val_loss: 3.0356 - val_accuracy: 0.0000e+00\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data: E:/sign_data/Dataset/train2/001\\1.jpg, Authenticity: Real\n",
      "Data: E:/sign_data/Dataset/train2/001\\2.jpg, Authenticity: Real\n",
      "Data: E:/sign_data/Dataset/train2/001\\3.jpg, Authenticity: Real\n",
      "Data: E:/sign_data/Dataset/train2/001\\4.jpg, Authenticity: Real\n",
      "Data: E:/sign_data/Dataset/train2/001\\5.jpg, Authenticity: Real\n",
      "Data: E:/sign_data/Dataset/train2/001_forg\\10.jpg, Authenticity: Forgery\n",
      "Data: E:/sign_data/Dataset/train2/001_forg\\6.jpg, Authenticity: Forgery\n",
      "Data: E:/sign_data/Dataset/train2/001_forg\\7.jpg, Authenticity: Forgery\n",
      "Data: E:/sign_data/Dataset/train2/001_forg\\8.jpg, Authenticity: Forgery\n",
      "Data: E:/sign_data/Dataset/train2/001_forg\\9.jpg, Authenticity: Forgery\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9990451\n",
      "0.00095491094\n",
      "[[9.9904507e-01 9.5491094e-04]]\n",
      "Signature authenticity: Real\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data: E:/sign_data/Dataset/train1/001\\10.png, Authenticity: Real\n",
      "Data: E:/sign_data/Dataset/train1/001\\6.png, Authenticity: Real\n",
      "Data: E:/sign_data/Dataset/train1/001\\7.png, Authenticity: Real\n",
      "Data: E:/sign_data/Dataset/train1/001\\KakaoTalk_20230524_170431079_10.png, Authenticity: Real\n",
      "Data: E:/sign_data/Dataset/train1/001\\KakaoTalk_20230524_170431079_11.png, Authenticity: Real\n",
      "Data: E:/sign_data/Dataset/train1/001_forg\\01_1.png, Authenticity: Forgery\n",
      "Data: E:/sign_data/Dataset/train1/001_forg\\01_2.png, Authenticity: Forgery\n",
      "Data: E:/sign_data/Dataset/train1/001_forg\\01_3.png, Authenticity: Forgery\n",
      "Data: E:/sign_data/Dataset/train1/001_forg\\01_4.png, Authenticity: Forgery\n",
      "Data: E:/sign_data/Dataset/train1/001_forg\\01_5.png, Authenticity: Forgery\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "###################### 추가 데이터 결과 확인 ######################\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import cv2\n",
    "from tensorflow.keras.models import model_from_json\n",
    "from tensorflow.keras.models import Model, load_model\n",
    "\n",
    "# Load the model architecture from JSON file\n",
    "with open('updated_model2.json', 'r') as json_file:\n",
    "    model_json = json_file.read()\n",
    "    model = model_from_json(model_json)\n",
    "\n",
    "# Load the model weights\n",
    "model.load_weights('updated_model2_weights.h5')\n",
    "\n",
    "# Load and preprocess new data\n",
    "SIZE = 224\n",
    "\n",
    "new_data_names = []\n",
    "new_data = []\n",
    "\n",
    "for per in os.listdir('E:/sign_data/Dataset/train1'):\n",
    "    for data in glob.glob('E:/sign_data/Dataset/train1/'+per+'/*.*'):\n",
    "        new_data_names.append(data)\n",
    "        img = cv2.imread(data)\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        img = cv2.resize(img, (SIZE, SIZE))\n",
    "        new_data.append(img)\n",
    "\n",
    "# Code for loading and preprocessing new data\n",
    "new_data = np.array(new_data) / 255.0\n",
    "\n",
    "# Reshape data\n",
    "new_data = new_data.reshape(-1, SIZE, SIZE, 3)\n",
    "\n",
    "# Predict the authenticity of the new data\n",
    "predictions = model.predict(new_data)\n",
    "authenticity = np.argmax(predictions, axis=1)\n",
    "\n",
    "# Display the results\n",
    "for i, data_name in enumerate(new_data_names):\n",
    "    print(f\"Data: {data_name}, Authenticity: {'Real' if authenticity[i] == 0 else 'Forgery'}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.52177495\n",
      "0.47822502\n",
      "[[0.52177495 0.47822502]]\n",
      "Signature authenticity: Forgery\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "################## new input verification #################\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import cv2\n",
    "from tensorflow.keras.models import model_from_json\n",
    "from tensorflow.keras.models import Model, load_model\n",
    "\n",
    "# Load the model architecture from JSON file\n",
    "with open('updated_model2.json', 'r') as json_file:\n",
    "    model_json = json_file.read()\n",
    "    model = model_from_json(model_json)\n",
    "\n",
    "# Load the model weights\n",
    "model.load_weights('updated_model2_weights.h5')\n",
    "\n",
    "\n",
    "# Load and preprocess the signature image\n",
    "SIZE = 224\n",
    "\n",
    "# Function to preprocess the image\n",
    "def preprocess_image(image_path):\n",
    "    img = cv2.imread(image_path)\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    img = cv2.resize(img, (SIZE, SIZE))\n",
    "    img = np.array(img) / 255.0\n",
    "    img = img.reshape(-1, SIZE, SIZE, 3)\n",
    "    return img\n",
    "\n",
    "# Function to predict the authenticity of a signature\n",
    "def predict_signature(image_path):\n",
    "    preprocessed_image = preprocess_image(image_path)\n",
    "    predictions = model.predict(preprocessed_image)\n",
    "    authenticity = \"Real\" if predictions[0][0] > predictions[0][1] and predictions[0][0] > 0.85  else \"Forgery\"\n",
    "    \n",
    "    print(predictions[0][0])\n",
    "    print(predictions[0][1])\n",
    "    \n",
    "    print(predictions)\n",
    "    \n",
    "    return authenticity\n",
    "\n",
    "# Path to the signature image\n",
    "\n",
    "#signature_image_path = 'E:/sign_data/Dataset/train1/001/5.png'\n",
    "\n",
    "signature_image_path = 'E:/sign_data/Dataset/forg.png'\n",
    "\n",
    "#signature_image_path = 'E:/sign_data/Dataset/KakaoTalk_20230528_151816148.png'\n",
    "\n",
    "#signature_image_path = 'E:/sign_data/KakaoTalk_20230528_152937618.jpg'\n",
    "\n",
    "# Predict the authenticity of the signature\n",
    "authenticity = predict_signature(signature_image_path)\n",
    "\n",
    "# Print the result\n",
    "print(\"Signature authenticity:\", authenticity)\n",
    "\n",
    "\n",
    "# Save model weights as HDF5\n",
    "model.save_weights(\"updated_model2_weights.h5\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 3 samples, validate on 2 samples\n",
      "Epoch 1/20\n",
      "3/3 [==============================] - 7s 2s/sample - loss: 43.4659 - accuracy: 0.0000e+00 - val_loss: 11.0277 - val_accuracy: 0.0000e+00\n",
      "Epoch 2/20\n",
      "3/3 [==============================] - 5s 2s/sample - loss: 12.8024 - accuracy: 0.0000e+00 - val_loss: 3.6866 - val_accuracy: 0.0000e+00\n",
      "Epoch 3/20\n",
      "3/3 [==============================] - 5s 2s/sample - loss: 2.1014 - accuracy: 0.0000e+00 - val_loss: 0.6392 - val_accuracy: 0.5000\n",
      "Epoch 4/20\n",
      "3/3 [==============================] - 5s 2s/sample - loss: 0.0751 - accuracy: 1.0000 - val_loss: 0.1478 - val_accuracy: 1.0000\n",
      "Epoch 5/20\n",
      "3/3 [==============================] - 5s 2s/sample - loss: 0.0242 - accuracy: 1.0000 - val_loss: 0.1010 - val_accuracy: 1.0000\n",
      "Epoch 6/20\n",
      "3/3 [==============================] - 5s 2s/sample - loss: 0.0206 - accuracy: 1.0000 - val_loss: 0.0935 - val_accuracy: 1.0000\n",
      "Epoch 7/20\n",
      "3/3 [==============================] - 5s 2s/sample - loss: 0.0231 - accuracy: 1.0000 - val_loss: 0.0791 - val_accuracy: 1.0000\n",
      "Epoch 8/20\n",
      "3/3 [==============================] - 5s 2s/sample - loss: 0.0253 - accuracy: 1.0000 - val_loss: 0.0618 - val_accuracy: 1.0000\n",
      "Epoch 9/20\n",
      "3/3 [==============================] - 5s 2s/sample - loss: 0.0243 - accuracy: 1.0000 - val_loss: 0.0521 - val_accuracy: 1.0000\n",
      "Epoch 10/20\n",
      "3/3 [==============================] - 5s 2s/sample - loss: 0.0226 - accuracy: 1.0000 - val_loss: 0.0426 - val_accuracy: 1.0000\n",
      "Epoch 11/20\n",
      "3/3 [==============================] - 5s 2s/sample - loss: 0.0208 - accuracy: 1.0000 - val_loss: 0.0313 - val_accuracy: 1.0000\n",
      "Epoch 12/20\n",
      "3/3 [==============================] - 5s 2s/sample - loss: 0.0184 - accuracy: 1.0000 - val_loss: 0.0223 - val_accuracy: 1.0000\n",
      "Epoch 13/20\n",
      "3/3 [==============================] - 5s 2s/sample - loss: 0.0151 - accuracy: 1.0000 - val_loss: 0.0155 - val_accuracy: 1.0000\n",
      "Epoch 14/20\n",
      "3/3 [==============================] - 5s 2s/sample - loss: 0.0118 - accuracy: 1.0000 - val_loss: 0.0108 - val_accuracy: 1.0000\n",
      "Epoch 15/20\n",
      "3/3 [==============================] - 5s 2s/sample - loss: 0.0088 - accuracy: 1.0000 - val_loss: 0.0074 - val_accuracy: 1.0000\n",
      "Epoch 16/20\n",
      "3/3 [==============================] - 4s 1s/sample - loss: 0.0062 - accuracy: 1.0000 - val_loss: 0.0052 - val_accuracy: 1.0000\n",
      "Epoch 17/20\n",
      "3/3 [==============================] - 4s 1s/sample - loss: 0.0041 - accuracy: 1.0000 - val_loss: 0.0035 - val_accuracy: 1.0000\n",
      "Epoch 18/20\n",
      "3/3 [==============================] - 5s 2s/sample - loss: 0.0026 - accuracy: 1.0000 - val_loss: 0.0022 - val_accuracy: 1.0000\n",
      "Epoch 19/20\n",
      "3/3 [==============================] - 4s 1s/sample - loss: 0.0016 - accuracy: 1.0000 - val_loss: 0.0013 - val_accuracy: 1.0000\n",
      "Epoch 20/20\n",
      "3/3 [==============================] - 4s 1s/sample - loss: 9.2979e-04 - accuracy: 1.0000 - val_loss: 7.6847e-04 - val_accuracy: 1.0000\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.layers import Flatten, Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.models import Model, load_model\n",
    "\n",
    "# Load existing model\n",
    "model = load_model('forge_real_signature_model2.h5')\n",
    "\n",
    "# Load and preprocess new data\n",
    "SIZE = 224\n",
    "\n",
    "new_data_names = []\n",
    "new_data = []\n",
    "new_labels = []\n",
    "\n",
    "for per in os.listdir('E:/sign_data/Dataset/train1'):\n",
    "    for data in glob.glob('E:/sign_data/Dataset/train1/'+per+'/*.*'):\n",
    "        new_data_names.append(data)\n",
    "        img = cv2.imread(data)\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        img = cv2.resize(img, (SIZE, SIZE))\n",
    "        new_data.append(img)\n",
    "        if per[-1] == 'g':\n",
    "            new_labels.append(1)\n",
    "        else:\n",
    "            new_labels.append(0)\n",
    "\n",
    "# Code for loading and preprocessing new data\n",
    "new_data = np.array(new_data) / 255.0\n",
    "new_labels = np.array(new_labels)\n",
    "\n",
    "# Convert labels to categorical\n",
    "new_labels = to_categorical(new_labels)\n",
    "\n",
    "# Reshape data\n",
    "new_data = new_data.reshape(-1, SIZE, SIZE, 3)\n",
    "\n",
    "# Specify the layers to be updated during transfer learning\n",
    "transfer_layers = 5  # Number of layers to freeze\n",
    "\n",
    "# Remove the last layer and add a new output layer\n",
    "x = model.layers[-2].output\n",
    "predictions = Dense(64, activation='relu')(x)\n",
    "predictions = Dense(2, activation='softmax')(predictions)\n",
    "transfer_model = Model(inputs=model.input, outputs=predictions)\n",
    "\n",
    "# Compile the model\n",
    "transfer_model.compile(loss='categorical_crossentropy', optimizer=Adam(lr=1e-4), metrics=['accuracy'])\n",
    "\n",
    "# Train the model with new data\n",
    "EPOCHS = 20\n",
    "BS = 64\n",
    "\n",
    "#early_stopping = EarlyStopping(monitor='val_loss', patience=5, verbose=1)\n",
    "\n",
    "progress = transfer_model.fit(new_data, new_labels, batch_size=BS, epochs=EPOCHS, validation_split=0.3)\n",
    "\n",
    "# Save the updated model\n",
    "transfer_model.save('updated_model2.h5')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data: E:/sign_data/Dataset/train1/001_forg\\10.png, Authenticity: Real\n",
      "Data: E:/sign_data/Dataset/train1/001_forg\\6.png, Authenticity: Real\n",
      "Data: E:/sign_data/Dataset/train1/001_forg\\7.png, Authenticity: Real\n",
      "Data: E:/sign_data/Dataset/train1/001_forg\\KakaoTalk_20230524_170431079_10.png, Authenticity: Real\n",
      "Data: E:/sign_data/Dataset/train1/001_forg\\KakaoTalk_20230524_170431079_11.png, Authenticity: Real\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "from tensorflow.keras.models import Model, load_model\n",
    "\n",
    "# Load existing model\n",
    "model = load_model('updated_model2.h5')\n",
    "\n",
    "# Load and preprocess new data\n",
    "SIZE = 224\n",
    "\n",
    "new_data_names = []\n",
    "new_data = []\n",
    "\n",
    "for per in os.listdir('E:/sign_data/Dataset/train1'):\n",
    "    for data in glob.glob('E:/sign_data/Dataset/train1/'+per+'/*.*'):\n",
    "        new_data_names.append(data)\n",
    "        img = cv2.imread(data)\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        img = cv2.resize(img, (SIZE, SIZE))\n",
    "        new_data.append(img)\n",
    "\n",
    "# Code for loading and preprocessing new data\n",
    "new_data = np.array(new_data) / 255.0\n",
    "\n",
    "# Reshape data\n",
    "new_data = new_data.reshape(-1, SIZE, SIZE, 3)\n",
    "\n",
    "# Predict the authenticity of the new data\n",
    "predictions = model.predict(new_data)\n",
    "authenticity = np.argmax(predictions, axis=1)\n",
    "\n",
    "# Display the results\n",
    "for i, data_name in enumerate(new_data_names):\n",
    "    print(f\"Data: {data_name}, Authenticity: {'Real' if authenticity[i] != 0 else 'Forgery'}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.00018663585\n",
      "0.9998134\n",
      "[[1.8663585e-04 9.9981338e-01]]\n",
      "Signature authenticity: Real\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "# Load the updated model\n",
    "model = load_model('updated_model2.h5')\n",
    "\n",
    "# Load and preprocess the signature image\n",
    "SIZE = 224\n",
    "\n",
    "# Function to preprocess the image\n",
    "def preprocess_image(image_path):\n",
    "    img = cv2.imread(image_path)\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    img = cv2.resize(img, (SIZE, SIZE))\n",
    "    img = np.array(img) / 255.0\n",
    "    img = img.reshape(-1, SIZE, SIZE, 3)\n",
    "    return img\n",
    "\n",
    "# Function to predict the authenticity of a signature\n",
    "def predict_signature(image_path):\n",
    "    preprocessed_image = preprocess_image(image_path)\n",
    "    predictions = model.predict(preprocessed_image)\n",
    "    authenticity = \"Real\" if predictions[0][1] > predictions[0][0] and predictions[0][1] > 0.85  else \"Forgery\"\n",
    "    \n",
    "    print(predictions[0][0])\n",
    "    print(predictions[0][1])\n",
    "    \n",
    "    print(predictions)\n",
    "    \n",
    "    return authenticity\n",
    "\n",
    "# Path to the signature image\n",
    "\n",
    "signature_image_path = 'E:/sign_data/Dataset/KakaoTalk_20230528_151816148.png'\n",
    "\n",
    "#signature_image_path = 'E:/sign_data/Dataset/001_forg/01_1.png'\n",
    "\n",
    "#signature_image_path = 'E:/sign_data/Dataset/KakaoTalk_20230528_151816148.png'\n",
    "\n",
    "#signature_image_path = 'E:/sign_data/KakaoTalk_20230528_152937618.jpg'\n",
    "\n",
    "# Predict the authenticity of the signature\n",
    "authenticity = predict_signature(signature_image_path)\n",
    "\n",
    "# Print the result\n",
    "print(\"Signature authenticity:\", authenticity)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.00036629924\n",
      "[[3.6629924e-04 9.9963367e-01]]\n",
      "Signature authenticity: Forgery\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "# Load the updated model\n",
    "model = load_model('updated_model2.h5')\n",
    "\n",
    "# Load and preprocess the signature image\n",
    "SIZE = 224\n",
    "\n",
    "# Function to preprocess the image\n",
    "def preprocess_image(image_path):\n",
    "    img = cv2.imread(image_path)\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    img = cv2.resize(img, (SIZE, SIZE))\n",
    "    img = np.array(img) / 255.0\n",
    "    img = img.reshape(-1, SIZE, SIZE, 3)\n",
    "    return img\n",
    "\n",
    "# Function to predict the authenticity of a signature\n",
    "def predict_signature(image_path):\n",
    "    preprocessed_image = preprocess_image(image_path)\n",
    "    predictions = model.predict(preprocessed_image)\n",
    "    authenticity = \"Real\" if predictions[0][0] >  0.85  else \"Forgery\"\n",
    "    \n",
    "    print(predictions[0][0])\n",
    "   \n",
    "    \n",
    "    print(predictions)\n",
    "    \n",
    "    return authenticity\n",
    "\n",
    "# Path to the signature image\n",
    "\n",
    "#signature_image_path = 'E:/sign_data/Dataset/train1/001/5.png'\n",
    "\n",
    "#signature_image_path = 'E:/sign_data/Dataset/001_forg/01_2.png'\n",
    "\n",
    "#signature_image_path = 'E:/sign_data/Dataset/KakaoTalk_20230528_151816148.png'\n",
    "\n",
    "signature_image_path = 'E:/sign_data/Dataset/1.png'\n",
    "\n",
    "# Predict the authenticity of the signature\n",
    "authenticity = predict_signature(signature_image_path)\n",
    "\n",
    "# Print the result\n",
    "print(\"Signature authenticity:\", authenticity)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The image is predicted as genuine.\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
